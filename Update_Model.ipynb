{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import eel\n",
    "\n",
    "emotions=[\"angry\", \"happy\", \"sad\", \"neutral\"]\n",
    "fishface = cv2.face.FisherFaceRecognizer_create()\n",
    "\n",
    "def update(emotions):\n",
    "    run_recognizer(emotions)\n",
    "    print(\"Saving model...\")\n",
    "    fishface.save(\"model.xml\")\n",
    "    print(\"Model saved!!\")\n",
    "\n",
    "# Data Loading    \n",
    "def make_sets(emotions):\n",
    "    training_data=[]\n",
    "    training_label=[]\n",
    "\n",
    "    for emotion in emotions:\n",
    "        training=training=sorted(glob.glob(\"dataset/%s/*\" %emotion))\n",
    "        for item in training:\n",
    "            image=cv2.imread(item)\n",
    "            gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            training_data.append(gray)\n",
    "            training_label.append(emotions.index(emotion))\n",
    "    return training_data, training_label\n",
    "\n",
    "# Saving  ML Model\n",
    "def run_recognizer(emotions):\n",
    "    training_data, training_label=make_sets(emotions)\n",
    "    print(\"Training model...\")\n",
    "    print(\"The size of the dataset is \"+str(len(training_data))+\" images\")\n",
    "    fishface.train(training_data, np.asarray(training_label))\n",
    "    \n",
    "    \n",
    "facedict={}\n",
    "video_capture=cv2.VideoCapture(0)\n",
    "\n",
    "def crop(clahe_image, face):\n",
    "    for (x, y, w, h) in face:\n",
    "        faceslice=clahe_image[y:y+h, x:x+w]\n",
    "        faceslice=cv2.resize(faceslice, (350, 350))\n",
    "        facedict[\"face%s\" %(len(facedict)+1)]=faceslice\n",
    "    return faceslice\n",
    "\n",
    "def grab_face():\n",
    "    ret, frame=video_capture.read()\n",
    "    cv2.imwrite('test.jpg', frame)\n",
    "    cv2.imwrite(\"images/main%s.jpg\" %count, frame)\n",
    "    gray=cv2.imread('test.jpg',0)\n",
    "    clahe=cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    clahe_image=clahe.apply(gray)\n",
    "    return clahe_image\n",
    "\n",
    "def detect_face():\n",
    "    clahe_image=grab_face()\n",
    "    face=facecascade.detectMultiScale(clahe_image, scaleFactor=1.1, minNeighbors=15, minSize=(10, 10), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    if len(face)>=1:\n",
    "        faceslice=crop(clahe_image, face)\n",
    "    else:\n",
    "        print(\"No/Multiple faces detected!!, passing over the frame\")\n",
    "\n",
    "def save_face(emotion):\n",
    "    print(\"\\n\\nLook \"+emotion+\" untill the timer expires and keep the same emotion for some time.\")\n",
    "    #winsound.Beep(frequency, duration)\n",
    "    print('\\a')\n",
    "    \n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        print(5-i)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    while len(facedict.keys())<16:\n",
    "        detect_face()\n",
    "\n",
    "    for i in facedict.keys():\n",
    "        path, dirs, files = next(os.walk(\"dataset/%s\" %emotion))\n",
    "        file_count = len(files)+1\n",
    "        cv2.imwrite(\"dataset/%s/%s.jpg\" %(emotion, (file_count)), facedict[i])\n",
    "    facedict.clear()\n",
    "\n",
    "def update_model(emotions):\n",
    "    print(\"Update mode for model is ready\")\n",
    "    checkForFolders(emotions)\n",
    "    \n",
    "    for i in range(0, len(emotions)):\n",
    "        save_face(emotions[i])\n",
    "    print(\"Collected the images, looking nice! Now updating the model...\")\n",
    "    Update_Model.update(emotions)\n",
    "    print(\"Model train successful!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facedict={}\n",
    "video_capture=cv2.VideoCapture(0)\n",
    "\n",
    "def crop(clahe_image, face):\n",
    "    for (x, y, w, h) in face:\n",
    "        faceslice=clahe_image[y:y+h, x:x+w]\n",
    "        faceslice=cv2.resize(faceslice, (350, 350))\n",
    "        facedict[\"face%s\" %(len(facedict)+1)]=faceslice\n",
    "    return faceslice\n",
    "\n",
    "def grab_face():\n",
    "    ret, frame=video_capture.read()\n",
    "    cv2.imwrite('test.jpg', frame)\n",
    "    cv2.imwrite(\"images/main%s.jpg\" %count, frame)\n",
    "    gray=cv2.imread('test.jpg',0)\n",
    "    clahe=cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    clahe_image=clahe.apply(gray)\n",
    "    return clahe_image\n",
    "\n",
    "def detect_face():\n",
    "    clahe_image=grab_face()\n",
    "    face=facecascade.detectMultiScale(clahe_image, scaleFactor=1.1, minNeighbors=15, minSize=(10, 10), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    if len(face)>=1:\n",
    "        faceslice=crop(clahe_image, face)\n",
    "    else:\n",
    "        print(\"No/Multiple faces detected!!, passing over the frame\")\n",
    "\n",
    "def save_face(emotion):\n",
    "    print(\"\\n\\nLook \"+emotion+\" untill the timer expires and keep the same emotion for some time.\")\n",
    "    #winsound.Beep(frequency, duration)\n",
    "    print('\\a')\n",
    "    \n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        print(5-i)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    while len(facedict.keys())<16:\n",
    "        detect_face()\n",
    "\n",
    "    for i in facedict.keys():\n",
    "        path, dirs, files = next(os.walk(\"dataset/%s\" %emotion))\n",
    "        file_count = len(files)+1\n",
    "        cv2.imwrite(\"dataset/%s/%s.jpg\" %(emotion, (file_count)), facedict[i])\n",
    "    facedict.clear()\n",
    "\n",
    "def update_model(emotions):\n",
    "    print(\"Update mode for model is ready\")\n",
    "    checkForFolders(emotions)\n",
    "    \n",
    "    for i in range(0, len(emotions)):\n",
    "        save_face(emotions[i])\n",
    "    print(\"Collected the images, looking nice! Now updating the model...\")\n",
    "    Update_Model.update(emotions)\n",
    "    print(\"Model train successful!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
